{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dW_5cqaLMqeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from geopy.distance import geodesic\n",
        "import os\n",
        "\n",
        "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"4\"\n",
        "\n",
        "train_data = pd.read_csv('/content/train_data.xlsx - Sheet1.csv')\n",
        "\n",
        "train_data = train_data.dropna(subset=['Lat', 'Long_'])\n",
        "\n",
        "outbreak_centroid = (train_data[['Lat', 'Long_']].median().values)\n",
        "train_data['Distance_to_Centroid'] = train_data.apply(\n",
        "    lambda row: geodesic((row['Lat'], row['Long_']), outbreak_centroid).km,\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "train_data['Confirmed_Cases'] = np.where(\n",
        "    (train_data['Deaths'].notna()) &\n",
        "    (train_data['Case_Fatality_Ratio'].notna()) &\n",
        "    (train_data['Case_Fatality_Ratio'] != 0),\n",
        "    (train_data['Deaths'] / train_data['Case_Fatality_Ratio']) * 100,\n",
        "    np.nan\n",
        ")\n",
        "\n",
        "train_data.loc[train_data['Case_Fatality_Ratio'] > 100, 'Case_Fatality_Ratio'] = np.nan\n",
        "\n",
        "imputer = IterativeImputer(random_state=42)\n",
        "features = ['Lat', 'Long_', 'Distance_to_Centroid']\n",
        "targets = ['Deaths', 'Case_Fatality_Ratio', 'Confirmed_Cases']\n",
        "impute_data = train_data[features + targets]\n",
        "imputed_data = pd.DataFrame(imputer.fit_transform(impute_data), columns=impute_data.columns)\n",
        "train_data[targets] = imputed_data[targets]\n",
        "\n",
        "X = train_data[features]\n",
        "y = train_data['Case_Fatality_Ratio']\n",
        "\n",
        "nan_indices = y.isna()\n",
        "if nan_indices.any():\n",
        "    print(f\"Dropping {nan_indices.sum()} rows with NaN in 'Case_Fatality_Ratio'.\")\n",
        "    X = X[~nan_indices]\n",
        "    y = y[~nan_indices]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_scaled = scaler.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [200, 300, 400],\n",
        "    'learning_rate': [0.05, 0.1, 0.2],\n",
        "    'max_depth': [4, 6, 8],\n",
        "    'min_samples_split': [2, 4, 6],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=GradientBoostingRegressor(random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train_scaled.ravel())\n",
        "\n",
        "cv_scores = cross_val_score(\n",
        "    grid_search.best_estimator_,\n",
        "    X_train,\n",
        "    y_train_scaled.ravel(),\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=5\n",
        ")\n",
        "cv_rmse = np.sqrt(-cv_scores)\n",
        "print(f\"Cross-Validation RMSE: {cv_rmse.mean():.4f} (±{cv_rmse.std():.4f})\")\n",
        "\n",
        "y_train_pred_scaled = grid_search.predict(X_train)\n",
        "y_train_pred_original = scaler.inverse_transform(y_train_pred_scaled.reshape(-1, 1))\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred_original))\n",
        "\n",
        "y_test_pred_scaled = grid_search.predict(X_test)\n",
        "y_test_pred_original = scaler.inverse_transform(y_test_pred_scaled.reshape(-1, 1))\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred_original))\n",
        "\n",
        "print(f\"Train RMSE: {train_rmse:.4f}\")\n",
        "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
        "feature_importances = grid_search.best_estimator_.feature_importances_\n",
        "feature_names = features\n",
        "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(importance_df)\n",
        "\n",
        "test_data = pd.read_csv('/content/test_points.xlsx - Sheet1.csv')\n",
        "\n",
        "test_data = test_data.dropna(subset=['Lat', 'Long_'])\n",
        "\n",
        "test_data['Distance_to_Centroid'] = test_data.apply(import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from geopy.distance import geodesic\n",
        "import os\n",
        "\n",
        "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"4\"\n",
        "\n",
        "train_data = pd.read_csv(\"C:/Users\\ARYAN\\Downloads/train_data.xlsx - Sheet1.csv\")\n",
        "\n",
        "train_data = train_data.dropna(subset=['Lat', 'Long_'])\n",
        "\n",
        "outbreak_centroid = (train_data[['Lat', 'Long_']].median().values)\n",
        "train_data['Distance_to_Centroid'] = train_data.apply(\n",
        "    lambda row: geodesic((row['Lat'], row['Long_']), outbreak_centroid).km,\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "train_data['Confirmed_Cases'] = np.where(\n",
        "    (train_data['Deaths'].notna()) &\n",
        "    (train_data['Case_Fatality_Ratio'].notna()) &\n",
        "    (train_data['Case_Fatality_Ratio'] != 0),\n",
        "    (train_data['Deaths'] / train_data['Case_Fatality_Ratio']) * 100,\n",
        "    np.nan\n",
        ")\n",
        "\n",
        "train_data.loc[train_data['Case_Fatality_Ratio'] > 100, 'Case_Fatality_Ratio'] = np.nan\n",
        "\n",
        "imputer = IterativeImputer(random_state=42)\n",
        "features = ['Lat', 'Long_', 'Distance_to_Centroid']\n",
        "targets = ['Deaths', 'Case_Fatality_Ratio', 'Confirmed_Cases']\n",
        "impute_data = train_data[features + targets]\n",
        "imputed_data = pd.DataFrame(imputer.fit_transform(impute_data), columns=impute_data.columns)\n",
        "train_data[targets] = imputed_data[targets]\n",
        "\n",
        "X = train_data[features]\n",
        "y = train_data['Case_Fatality_Ratio']\n",
        "\n",
        "nan_indices = y.isna()\n",
        "if nan_indices.any():\n",
        "    print(f\"Dropping {nan_indices.sum()} rows with NaN in 'Case_Fatality_Ratio'.\")\n",
        "    X = X[~nan_indices]\n",
        "    y = y[~nan_indices]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_scaled = scaler.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [300, 400, 500],\n",
        "    'learning_rate': [0.05, 0.1, 0.15],\n",
        "    'max_depth': [6, 8, 10],\n",
        "    'min_samples_split': [2, 4, 8],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=GradientBoostingRegressor(random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train_scaled.ravel())\n",
        "\n",
        "cv_scores = cross_val_score(\n",
        "    grid_search.best_estimator_,\n",
        "    X_train,\n",
        "    y_train_scaled.ravel(),\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=5\n",
        ")\n",
        "cv_rmse = np.sqrt(-cv_scores)\n",
        "print(f\"Cross-Validation RMSE: {cv_rmse.mean():.4f} (±{cv_rmse.std():.4f})\")\n",
        "\n",
        "y_train_pred_scaled = grid_search.predict(X_train)\n",
        "y_train_pred_original = scaler.inverse_transform(y_train_pred_scaled.reshape(-1, 1))\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred_original))\n",
        "\n",
        "y_test_pred_scaled = grid_search.predict(X_test)\n",
        "y_test_pred_original = scaler.inverse_transform(y_test_pred_scaled.reshape(-1, 1))\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred_original))\n",
        "\n",
        "print(f\"Train RMSE: {train_rmse:.4f}\")\n",
        "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
        "feature_importances = grid_search.best_estimator_.feature_importances_\n",
        "feature_names = features\n",
        "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(importance_df)\n",
        "\n",
        "test_data = pd.read_csv(\"C:/Users\\ARYAN\\Downloads/test_points.xlsx - Sheet1.csv\")\n",
        "\n",
        "test_data = test_data.dropna(subset=['Lat', 'Long_'])\n",
        "\n",
        "test_data['Distance_to_Centroid'] = test_data.apply(\n",
        "    lambda row: geodesic((row['Lat'], row['Long_']), outbreak_centroid).km,\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "X_test_data = test_data[features]\n",
        "test_data['Predicted_CFR'] = scaler.inverse_transform(grid_search.predict(X_test_data).reshape(-1, 1))\n",
        "\n",
        "X_deaths = train_data[features]\n",
        "y_deaths = train_data['Deaths']\n",
        "\n",
        "nan_indices = y_deaths.isna()\n",
        "if nan_indices.any():\n",
        "    print(f\"Dropping {nan_indices.sum()} rows with NaN in 'Deaths'.\")\n",
        "    X_deaths = X_deaths[~nan_indices]\n",
        "    y_deaths = y_deaths[~nan_indices]\n",
        "\n",
        "deaths_model = GradientBoostingRegressor(n_estimators=400, learning_rate=0.05, max_depth=8, random_state=42)\n",
        "deaths_model.fit(X_deaths, y_deaths)\n",
        "\n",
        "test_data['Predicted_Deaths'] = deaths_model.predict(X_test_data)\n",
        "\n",
        "test_data['Confirmed_Cases'] = (test_data['Predicted_Deaths'] / test_data['Predicted_CFR']) * 100  # CFR = (Deaths / Confirmed) * 100\n",
        "\n",
        "test_data[['Lat', 'Long_', 'Predicted_CFR', 'Predicted_Deaths', 'Confirmed_Cases']].to_csv(\"C:\\diploma\\predictions.csv.csv\", index=False)\n",
        "\n",
        "print(\"Predictions saved to 'test_pred.csv'\")\n",
        "\n",
        "    lambda row: geodesic((row['Lat'], row['Long_']), outbreak_centroid).km,\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "X_test_data = test_data[features]\n",
        "test_data['Predicted_CFR'] = scaler.inverse_transform(grid_search.predict(X_test_data).reshape(-1, 1))\n",
        "\n",
        "X_deaths = train_data[features]\n",
        "y_deaths = train_data['Deaths']\n",
        "\n",
        "nan_indices = y_deaths.isna()\n",
        "if nan_indices.any():\n",
        "    print(f\"Dropping {nan_indices.sum()} rows with NaN in 'Deaths'.\")\n",
        "    X_deaths = X_deaths[~nan_indices]\n",
        "    y_deaths = y_deaths[~nan_indices]\n",
        "\n",
        "deaths_model = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, max_depth=6, random_state=42)\n",
        "deaths_model.fit(X_deaths, y_deaths)\n",
        "\n",
        "test_data['Predicted_Deaths'] = deaths_model.predict(X_test_data)\n",
        "\n",
        "test_data['Confirmed_Cases'] = (test_data['Predicted_Deaths'] / test_data['Predicted_CFR']) * 100  # CFR = (Deaths / Confirmed) * 100\n",
        "\n",
        "test_data[['Lat', 'Long_', 'Predicted_CFR', 'Predicted_Deaths', 'Confirmed_Cases']].to_csv('test_pred.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved to 'test_pred.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQEwQk7AK1Hm",
        "outputId": "d85516ea-e742-43f7-97ce-f8a80492a3b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropping 88 rows with NaN in 'Case_Fatality_Ratio'.\n",
            "Cross-Validation RMSE: 0.1028 (±0.0047)\n",
            "Train RMSE: 0.6588\n",
            "Test RMSE: 0.8393\n",
            "\n",
            "Feature Importances:\n",
            "                Feature  Importance\n",
            "0                   Lat    0.362801\n",
            "1                 Long_    0.335968\n",
            "2  Distance_to_Centroid    0.301231\n",
            "Dropping 88 rows with NaN in 'Deaths'.\n",
            "Predictions saved to 'test_pred.csv'\n"
          ]
        }
      ]
    }
  ]
}